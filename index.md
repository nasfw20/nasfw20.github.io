---
layout: default
---

Over the past few years, deep learning (DL) has helped advance computer vision by leaps and bounds. From achieving super-human accuracy in image classification tasks to dramatically improving image generation, deep learning based algorithms have dominated the performance charts and established state-of-the-art time after time. Although deep learning has helped in minimizing the reliance on domain specific hand-crafted features, it is in fact the hundreds of hours of human machine learning expertise that goes into squeezing the last bit of performance from these models. From doing data processing to taking several decisions on choosing the right model architecture and associated hyperparameters, deep learning is still heavily dependent on humans to achieve desired results. This dependence limits the application of deep learning in several domains especially the non-technical ones such as health, education and retail, where human expertise in deep learning may not be readily available. Automated Machine Learning, which incorporates methods such as neural architecture search (NAS) and hyperparameter optimization (HPO), provides approaches/systems to help deep learning be used for various applications without any expert knowledge of deep learning. It can help democratize deep learning by reducing the need for DL expertise in application development. 

# Call for Papers

Recent years have witnessed a significant rise in research related to NAS that allows automatically finding deep network architectures. These architectures often achieve better performance than the state-of-the-art methods that have been carefully designed by DL researchers. Although NAS shows promise by exhibiting superior performance on standard benchmarks such as CIFAR-10/100 and ImageNet, the evidence is scarce that they would work equally well on real-world datasets. Moreover, the research has rarely explored vision-based tasks such as pose estimation, activity recognition in videos, generative models, vision-language tasks and real-time vision applications. This gap between published literature for NAS and their performance on real-world datasets/applications is yet to be addressed. The aim of this workshop is to advocate NAS for in-the-wild computer vision across this wide range of tasks and potentially across a range of computing platforms.

The workshop scope includes (but is not limited to),

* Neural architecture search
*	Challenges in using NAS/HPO for real-world unconstrained datasets and applications
*	Application of NAS/HPO in real-time vision applications
*	Application of NAS/HPO beyond image classification and object detection
*	Meta learning and transfer learning for computer vision
*	Learning to learn for computer vision.

## Header 2

> This is a blockquote following a header.
>
> When something is important enough, you do it even if the odds are not in your favor.

### Header 3

```js
// Javascript code with syntax highlighting.
var fun = function lang(l) {
  dateformat.i18n = require('./lang/' + l)
  return true;
}
```

```ruby
# Ruby code with syntax highlighting
GitHubPages::Dependencies.gems.each do |gem, version|
  s.add_dependency(gem, "= #{version}")
end
```

#### Header 4

*   This is an unordered list following a header.
*   This is an unordered list following a header.
*   This is an unordered list following a header.

##### Header 5

1.  This is an ordered list following a header.
2.  This is an ordered list following a header.
3.  This is an ordered list following a header.

###### Header 6

| head1        | head two          | three |
|:-------------|:------------------|:------|
| ok           | good swedish fish | nice  |
| out of stock | good and plenty   | nice  |
| ok           | good `oreos`      | hmm   |
| ok           | good `zoute` drop | yumm  |

### There's a horizontal rule below this.

* * *

### Here is an unordered list:

*   Item foo
*   Item bar
*   Item baz
*   Item zip

### And an ordered list:

1.  Item one
1.  Item two
1.  Item three
1.  Item four

### And a nested list:

- level 1 item
  - level 2 item
  - level 2 item
    - level 3 item
    - level 3 item
- level 1 item
  - level 2 item
  - level 2 item
  - level 2 item
- level 1 item
  - level 2 item
  - level 2 item
- level 1 item

### Small image

![Octocat](https://github.githubassets.com/images/icons/emoji/octocat.png)

### Large image

![Branching](https://guides.github.com/activities/hello-world/branching.png)


### Definition lists can be used with HTML syntax.

<dl>
<dt>Name</dt>
<dd>Godzilla</dd>
<dt>Born</dt>
<dd>1952</dd>
<dt>Birthplace</dt>
<dd>Japan</dd>
<dt>Color</dt>
<dd>Green</dd>
</dl>

```
Long, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.
```

```
The final element.
```
